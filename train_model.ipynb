{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset (example: FakeNewsNet)\n",
    "texts, labels = load_dataset()  # Assume this function loads your data\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convert to TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train\n",
    ")).shuffle(100).batch(16)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test\n",
    ")).batch(16)\n",
    "\n",
    "# Load and train model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_dataset, epochs=3, validation_data=test_dataset)\n",
    "model.save('models/fake_news_model.h5')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
